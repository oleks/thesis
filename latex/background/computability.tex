\chapter{Computability} \label{sec:background-computability}

\begin{quotation}

\footnotesize\sffamily\itshape

\begin{flushright}

The ``computable'' numbers may be described briefly as the real numbers whose
expressions as a decimal are calculable by finite means.

\smallbreak

\upshape

--- ALAN TURING, {\itshape Proceedings of The London Mathematical Society} (1936-7)

\end{flushright}

\end{quotation}

The theory of computability is concerned with the nature of computation. It
originated in the 1930s with the celebrated works of G\"odel, Kleene, Church,
Post, Turing, et al.  Fundamental to the field is the Church-Turing thesis,
which proposes that the natural phenomena of computation is captured by the
notion of computation with a Turing machine.

We begin with a brief discussion about the intuitive notion of computation,
leading way for a formal\footnote{By ``formal'', in this chapter and the next,
we mean built atop the mathematical foundations as outlined in
\refSection{preface:mathematical-foundations}. It is an interesting thesis in
its own right to consider an exposition of computability and complexity theory
as based solely on the Church-Turing thesis as an axiom. Some work in this
direction appears in \cite{markov-1954}.} definition of computation with a
Turing machine, and an exposition of the Church-Turing thesis.

\begin{notion} A problem is ``computable'' if it can be solved by transforming
a mathematical object, without ingenuity.
\end{notion}

Any attempt at a more definite notion of computability seems to arrive at a
philosophical impasse, where the notions of ``mathematical object'',
``transformation'', and ``ingenuity'' form a philosophical conundrum. The
indefinite notion however, is sufficient to state the following hypothesis:

\begin{hypothesis} \label{ths:problem-composition} If a problem $P$ can be
solved by solving a computable problem $Q$, immediately followed by solving a
computable problem $R$, then $P$ itself is computable. \end{hypothesis}

The hypothesis clearly holds as nothing is done other than solve the two
computable problems\footnote{Although, doing nothing, is itself a philosophical
conundrum.}.  So any problem which can be solved by solving a finite sequence
of computable problems, is itself computable. We now arrive at an intuitive
notion of an algorithm:

\begin{notion} \label{ntn:problem-algorithm} An ``algorithm'' is a
specification of how a problem can be solved by solving a finite sequence of
computable problems, called ``steps''.\end{notion}

Such indefinite notions are useful for little else. We are left to fall prey to
some choice of formalism. To this end, the notion of computation with a Turing
machine, due to Alan Turing\cite{turing-1936-7}, forms perhaps the most
classical conceptualization of computation. The conceptualization below is
heavily inspired by the those that appear in \cite{tourlakis-1984},
\cite{jones-1997}, \cite{homer-selman-2011}, and \cite{sipser-2013}, but
slightly differs from all of them.

A \textbf{Turing machine} is a symbol-manipulating machine, working with a
1-dimensional, unbounded tape divided into discrete, equally sized squares.
Each square is occupied by a symbol drawn from the \textbf{tape alphabet} of
the machine.  The tape alphabet consists of the \textbf{input/output alphabet},
usually denoted $\Sigma$, and a special \textbf{blank symbol}, written \blank,
which is not in $\Sigma$. The tape has a left edge, but extends into infinity
to the right, filled with blank symbols.

% Inspired by
% http://tex.stackexchange.com/questions/203257/tikz-chains-with-one-side-of-the-leftmost-node-thickbold

\begin{figure}[h!]
\centering
\begin{tikzpicture}
  \tikzset{tape/.style={minimum size=.7cm, draw}}
  \begin{scope}[start chain=0 going right, node distance=0mm]
   \foreach \x [count=\i] in {\symb{1}, \symb{0}, \symb{1}, \symb{0}, \symb{1}, \symb{0}, \blank, \blank, \blank} {
    \ifnum\i=9 % if last node reset outer sep to 0pt
      \node [on chain=0, tape, outer sep=0pt] (n\i) {\x};
      \draw (n\i.north east) -- ++(.1,0) decorate [decoration={zigzag, segment length=.12cm, amplitude=.02cm}] {-- ($(n\i.south east)+(+.1,0)$)} -- (n\i.south east) -- cycle;
     \else
      \node [on chain=0, tape] (n\i) {\x};
     \fi
   }
   \node [right=.25cm of n9] {$\cdots$};
  \end{scope}
\end{tikzpicture}
\caption[]{Schematic of a Turing tape with $\Sigma=\set{\symb{0},\symb{1}}$.}
\label{fig:turing-tape}
\end{figure}

A Turing machine is always in one of a finite, nonempty set of \textbf{states},
also called the \textbf{state alphabet}, and has a tape head, always
overlooking a particular square on the tape. A Turing machine ``computes'' by
transitioning between states and working with the tape head.

The transition from one state to the next is uniquely determined by the current
state, and the symbol read off the overlooked square. A state transition bears
with it writing a new symbol to the overlooked square, and moving the tape head
one square to the left, or one square to the right.

There are two distinguished states, the \textbf{starting state}, usually
denoted \qstart, and the \textbf{halting state}, usually denoted \qhalt. A
Turing machine \textbf{starts} a computation by entering the state \qstart,
overlooking the square at the left end of the tape. A Turing machine
\textbf{halts}, i.e.  performs no further state transitions, if it ever
transitions to the state \qhalt. A halting computation is a finite sequence of
state transitions.

We now formalise these notions as follows:

\begin{definition} A Turing machine (TM) $M$ is a 5-tuple $\p{Q, \qstart,
\qhalt, \Gamma, \delta}$, where

\begin{enumerate}

\item $Q$ is the state alphabet,

\item $\qstart \in Q$ is the starting state,

\item $\qhalt \in Q$ is the halting state,

\item $\Gamma = \Sigma \cup \set{\blank}$ is the tape alphabet, where $\Sigma$
is the input/output alphabet, and $\Sigma \cap \set{\blank} = \emptyset$

\item $\delta : Q \setminus \set{\qhalt} \times \Gamma \rightarrow Q \times
\Gamma \times \set{\symb{L}, \symb{R}}$ is the (total) transition function.

\end{enumerate}

We will omit stating the components of a TM, when they are clear from context.

\end{definition}

\begin{definition} An $M$-\textbf{configuration} of a TM $M$, is a 3-tuple,
written $uqv$, where $u\in\Gamma^*$, $q\in Q$, and $v\in\Gamma^*$. If
$v=\varepsilon$, it can be regarded as if $v=\blank$. \end{definition}

We will omit stating that $u\in\Gamma^*$, $q\in Q$, and $v\in\Gamma^*$ when
this is clear from context. The additional permission to regard an empty $v$ as
a $v$ containing a blank symbol facilitates the notion of an infinite tape.
This is illustrated by an example further below.

\begin{definition} An $M$-configuration $C_x$ \textbf{yields} an
$M$-configuration $C_y$, written $C_x\leadsto C_y$, iff either

\begin{enumerate}

\item $\delta\p{q_x,a}=\p{q_y,b,\goright}$, $C_x=uq_xav$, and $C_y=ubq_yv$,

\item $\delta\p{q_x,a}=\p{q_y,b,\goleft}$, $C_x=ucq_xav$, and $C_y=uq_ycbv$,
or

\item $\delta\p{q_x,a}=\p{q_y,b,\goleft}$, $C_x=q_xbv$,  and $C_y=q_ybv$;

where $a,b,c\in\Sigma$, and $q_x,q_y\in Q$.

\end{enumerate}

\end{definition}

\begin{definition}

A TM $M$ halts with output $y\in\Sigma^*$ on input $x\in\Sigma^*$, written
$M\p{x}\downarrow y$, iff there exists a finite sequence of $M$-configurations,
$\chev{C_1,C_2,\ldots, C_k}$, of length $k\in\mathbb{N}$, such that

\begin{enumerate}

\item $C_1=\qstart x$,

\item $C_i\leadsto C_{i+1}$ for all $1\leq i < k$,

\item $C_k=u\qhalt yv$.

\end{enumerate}

\end{definition}

Note, in particular that the ``tape'' of a TM in such a formalisation is not
infinite, and rather, finite but extensible. As an example, consider a TM $M$,
where $\Sigma=\set{\symb{\#}}$ and $\delta\p{\qstart, \blank} =
\p{\qhalt,\#,\goleft}$. We have that $\qstart\leadsto\qhalt\#$ and
$M\p{}\downarrow\#$.

\section{Minor variants}

In this section, we give some justification for our definition of Turing
machines by considering how it can simulate various variants often found in the
literature. The ability of our Turing machines to simulate these variants
builds confidence in our definition, and permits us to stand on the shoulders
of giants, and state the Church-Turing thesis based on our notion of Turing
machines.

\subsection{Don't move the tape head}

In our definition of a Turing machine, each transition bore with it the
movement of the tape head either left or right. Some variants will also permit
the tape head to stay where it is. 

Our machines can simulate such machines by having two state transitions, first
moving left, then right whenever we wish to simulate a state transition where
the tape head does not move.

\subsection{A larger tape alphabet}

In our definition of a Turing machine we defined the tape alphabet as $\Gamma =
\Sigma \cup \set{\blank}$, where $\Sigma$ is the input/output alphabet and
$\Sigma \cap \set{\blank} = \emptyset$. It is frequent in literature to relax
this definition, and let $\Gamma \supseteq \Sigma \cup \set{\blank}$.

A larger tape alphabet permits more eloquent work with the tape, where e.g. the
tape alphabet could be extended with a particularly decorated version of the
input/output alphabet. This is useful when e.g. showing that a one-tape Turing
machine is equivalent in computational power to a multi-tape Turing machine.

In a vein similar to Turing machines where the tape head can stay, we can
simulate a machine where

\subsection{Multiple halting states}

\subsection{Multiway and multiple tapes}

\section{Major variants}

Major variants differ from minor variants in that they tend to play a major
role in the theory of computability and complexity.

\begin{itemize}

\item Nondeterministic Turing machines.

\item Enumerators.

\end{itemize}

\section{Church-Turing thesis}

\begin{hypothesis} \textit{Church-Turing thesis.}

No intuitive notion of an algorithm can compute a function that a Turing
machine cannot compute.

\end{hypothesis}

The Church-Turing thesis does not lend itself to mathematical proof. Rather, it
is a proposed explanation of the natural phenomena of computation. The
hypothesis remains to be disproven, and in the mean-while, computational models
more convenient than the Turing machine have emerged, some of them, even useful
in practice.

Some of these models have a formal specification, and so it lends itself to
formal proof that these models compute no more than a Turing machine does.

Our belief in the Church-Turing thesis has however, a notational advantage: to
show that a function $f$ is computable, it is sufficient to informally
describe, an algorithm for computing $f$. Due to the hypothesis, any
formalisation of the informal algorithm, will yield a computable algorithm,

This is sometimes called providing the ``pseudo-code'' for computing $f$, but can be said
of most ``code'' today in general.

Formal methods, due to their inherent complexity are seldom employed.

\section{What does a Turing machine compute?}\label{sec:background-what-tm-computes}

The introduction to this chapter talked about ``problems'' being computable. We
went on to define computation by means of a Turing machine. We now formally
discuss what it is that a Turing machine computes, effectively giving a
definition of what we mean by the notion of a ``problem''.

Consider a Turing machine $M$. The possible inputs and outputs of $M$ form a
set, in particular, the set $\Sigma^*$. Given a particular input $x\in\Sigma^*$
on tape, a Turing machine either halts with a particular output $y\in\Sigma^*$
on tape, or loops forever. The behaviour of $M$ is completely deterministic,
that is, $M$ always behaves the same way for the same inputs. This summons the
properties PF-1, PF-2, and PF-3 of the specification of partial functions (see
also ...), leading way to the following definition:

\begin{definition}\label{def:tm-computes-sigma} A TM $M$ \textbf{computes} a
partial function $f:\Sigma^*\rightarrow \Sigma^*_\bot$. \end{definition}

This definition also satisfies the properties PF-4 and PF-5. PF-4 holds as both
$\Sigma^*$, and $f\p{x}$ for every $x$ are completely determined by $M$. PF-5
holds as we can deduce a definition of $M$ if given the set of tuples defining
$f$.

The output of one Turing machine can serve as the input of another, and so the
functions that the two Turing machines compute are composable.

\begin{theorem} If a TM $M_1$ computes a function $f : \Sigma^* \rightarrow
\Sigma^*_\bot$, and a TM $M_2$ computes a function $g : \Sigma^* \rightarrow
\Sigma^*_\bot$, then there exists a TM $M$ which computes the function $g\circ
f : \Sigma^* \rightarrow \Sigma^*_\bot$.\end{theorem}

\begin{proof} Proof by pseudo code. \end{proof}

We can now parley about our algorithms by the compositions of functions that
our Turing machines compute, rather than resort to talking about the Turing
machines themselves.

Although this characterisation may be sufficient for proofs in computability
theory, it is of little comfort to the eloquent programmer. Turing machines
were not designed for eloquent programming, and so this is to be expected.  We
can, never-the-less, provide a slightly more ``abstract'' characterisation of
what it is that a Turing machine computes.

% and hereby lift the design of programming languages off the ground.

\begin{theorem}\label{thm:unique-representation} For any countable set $X$, and
alphabet $\Sigma$, for each $x\in X$ there is a \textbf{unique representation}
of $x$ in $\Sigma^*$, written $\overline{x}\in\Sigma^*$. That is, there is an
injective function $h:X\rightarrow \Sigma^*$. \end{theorem}

\begin{proof} By ... we have an injective function $f:X\rightarrow \mathbb{N}$.
By ... we have a bijective function $g:\mathbb{N}\rightarrow\Sigma^*$. Let
$h\triangleq g\circ f$. By ...  $h : X\rightarrow \Sigma^*$ is an injective
function.  \end{proof}

By \refDef{tm-computes-sigma} and \refTheorem{unique-representation}, we can
now say more generally, that a Turing machine computes a function
$f:X\rightarrow \Sigma^*$, for some countable set $X$. This is better, but a
little disappointing. It is a bit like saying if you throw in the ingredients
of a cake, we promise to deliver you a string of symbols drawn from $\Sigma$,
if anything. Recall that $\Sigma^*$ is a countably infinite set. This leads to
the slightly more general characterisation of what it is that a TM computes.

\begin{corollary} A TM computes a partial function $f:X\rightarrow Y_\bot$, for
some countable set $X$, and countably infinite set $Y$. \end{corollary}

\begin{proof} Let $\Sigma$ be the alphabet of some Turing machine $M$. By
\refTheorem{unique-representation} there is an injective function $i : X
\rightarrow \Sigma^*$. By .. there is a bijective function $g : \Sigma^*_\bot
\rightarrow Y_\bot$. Without loss of generality, let $M$ compute the function
$h : \Sigma^* \rightarrow \Sigma^*_\bot$, and let $f = g \circ h \circ i$.
\end{proof} 

\begin{remark} We have now completely dropped the components of a TM from the
characterisation of the function that the TM computes, attaining hereby a
``machine-independent characterisation'' of what it is that the TM computes.
\end{remark}

This is still a little disappointing. A countably infinite set is ``no
smaller'' than $\Sigma^*$. We can now throw in the ingredients of a cake, and
be guaranteed only to get \emph{some} cake, if anything at all. We are not
guaranteed to get a cake, and certainly not guaranteed to get the cake we want.

It is perhaps worthwhile to reconsider representing functions as sets. If for a
given TM $M$, we could construct the set of pairs
$\set{\p{x,y}\st{\text{$M\p{x}\downarrow y$ or $y=\bot$}}}$, the second
components of the pairs uniquely determine the codomain of the function
computed by $M$. As we shall later prove, deciding whether a Turing machine
halts or not, is ``undecidable'' in general, and so unfortunately, this is not
an option.

It is the subject matter of much Computer Science, and likewise the remainder
of this thesis, to consider limiting the programmers to talk just in terms of
composing particular functions, for which the codomains (among other
properties) are either known beforehand, or can be determined by finite means.
Hopefully, we can then be guaranteed, that if we throw in the right
ingredients, without too much hassle, we will be delivered just the cake we
want, in time for lunch.

\subsection{Recursively enumerable and recursive functions}

\begin{definition} The partial function $f:X\rightarrow Y_\bot$ is said to be
\textbf{recursively enumerable} if some TM computes it. \end{definition}

\begin{definition} The (total) function $f:X\rightarrow Y$ is said to be
\textbf{recursive} if some TM computes it. \end{definition}

\section{Some uncomputable problems}
